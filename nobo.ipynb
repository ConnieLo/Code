{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 4 ... 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "#separate data into text and labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('text.csv')\n",
    "\n",
    "text = []\n",
    "labels = []\n",
    "for row in df.iterrows():\n",
    "    text.append(row[1]['text'])\n",
    "    labels.append(row[1]['label'])\n",
    "\n",
    "labels = np.array(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import preprocessing module(s), tokenise etc\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "'''tokens = []\n",
    "for s in text:\n",
    "    s.maketrans('','', string.punctuation)\n",
    "    token = word_tokenize(s)\n",
    "    for t in token:\n",
    "        if t in set(stopwords.words('english')):\n",
    "            token.remove(t)\n",
    "    tokens.append(token)\n",
    "\n",
    "with open('tokens', 'wb') as f:\n",
    "    pickle.dump(tokens, f)\n",
    "\n",
    "'''\n",
    "with open ('tokens', 'rb') as f: #so the process need not be repeated in the future\n",
    "    tokens = pickle.load(f)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "    \n",
    "'''tagged = []\n",
    "for sent in tokens:\n",
    "    tagged.append(pos_tag(sent))\n",
    "wordnet_tagged = []\n",
    "for sent in tagged:\n",
    "    wordnet_tagged.append(list(map(lambda x: (x[0], pos_tagger(x[1])), sent)))\n",
    "print(wordnet_tagged)'''\n",
    "\n",
    "with open('wn_tagged', 'rb') as f:\n",
    "    wordnet_tagged = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "'''lemmatised = [] #lemmatization\n",
    "for sent in wordnet_tagged:\n",
    "    L = []\n",
    "    for i in sent:\n",
    "        if i[1] == None:\n",
    "            L.append(lemma.lemmatize(i[0]))\n",
    "        else:\n",
    "            L.append(lemma.lemmatize(*i))\n",
    "    lemmatised.append(L)'''\n",
    "\n",
    "with open('lemmatised', 'rb') as f:\n",
    "    lemmatised = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training, validation and test sentences before training **USING SMALLER DATASET AS CANNOT ALLOCATE RESOURCES**\n",
    "sents_train, sents_val, sents_test = lemmatised[:round(len(lemmatised)*0.1)], lemmatised[round(len(lemmatised)*0.1):round(len(lemmatised)*0.12)], lemmatised[round(len(lemmatised)*0.12):round(len(lemmatised)*0.14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectoriser():\n",
    "\n",
    "    def __init__(self, corpus=None):\n",
    "        self.word_set = {}\n",
    "        if corpus:\n",
    "            self.fit(corpus)\n",
    "    \n",
    "    def fit(self, corpus): #learns vocabulary of given corpus\n",
    "        ws = self.word_set\n",
    "        for d in corpus:\n",
    "            for t in d:\n",
    "                if t not in ws:\n",
    "                    ws[t] = len(ws)\n",
    "        self.word_set = ws\n",
    "    \n",
    "    def transform(self, doc): #returns feature vector for given document based on learned vocabulary\n",
    "        vec = np.zeros([len(self.word_set)], dtype=np.short) #generates vector of zeroes the same length as learned vocabulary\n",
    "        for t in doc:\n",
    "            if t in self.word_set:\n",
    "                vec[self.word_set[t]] += 1 #for every instance of a known word, add 1 to corresponding position in vector\n",
    "        return(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "VVV = Vectoriser(sents_train) #fits vectoriser to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = np.array([VVV.transform(x) for x in sents_train], dtype=np.byte) #small datatype so that entire array can be created\n",
    "#val_vecs = np.array([VVV.transform(x) for x in sents_val], dtype=np.short)\n",
    "#test_vecs = np.array([VVV.transform(x) for x in sents_test], dtype=np.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "y_train = labels[:round(len(labels)*0.1)]\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(train_vecs, y_train)\n",
    "\n",
    "with open('trained_NB.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055422264875239\n"
     ]
    }
   ],
   "source": [
    "y_val = labels[round(len(labels)*0.1):round(len(labels)*0.12)]\n",
    "val_vecs = np.array([VVV.transform(x) for x in sents_val], dtype=np.byte)\n",
    "preds = classifier.predict(val_vecs)\n",
    "\n",
    "total = 0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_val[i]:\n",
    "        total += 1\n",
    "\n",
    "accuracy = total/len(preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of first Naive bayes implementation: 80.55% (2 s.f.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055422264875239\n"
     ]
    }
   ],
   "source": [
    "val_pns = [[0,0,0] for i in range(6)]# 0: TP, 1: FP, 2: FN, TN can be inferred using other class TPs\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_val[i]:\n",
    "        val_pns[preds[i]][0] += 1 #increase TP count on predicted/true class\n",
    "    else:\n",
    "        val_pns[y_val[i]][2] += 1 #increase FN count on true class\n",
    "        val_pns[preds[i]][1] += 1 #increase FP count on predicted class\n",
    "\n",
    "#Going to use micro avg first\n",
    "total_tps = np.sum([x[0] for x in val_pns])\n",
    "total_fps = np.sum([x[1] for x in val_pns])\n",
    "total_fns = np.sum([x[2] for x in val_pns])\n",
    "\n",
    "precision = total_tps/(total_tps+total_fps)\n",
    "recall = total_tps/(total_tps+total_fns)\n",
    "\n",
    "fscore = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(fscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
