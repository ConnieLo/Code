{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 4 ... 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "#separate data into text and labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('text.csv')\n",
    "\n",
    "text = []\n",
    "labels = []\n",
    "for row in df.iterrows():\n",
    "    text.append(row[1]['text'])\n",
    "    labels.append(row[1]['label'])\n",
    "\n",
    "labels = np.array(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import preprocessing module(s), tokenise etc\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "'''tokens = []\n",
    "for s in text:\n",
    "    s.maketrans('','', string.punctuation)\n",
    "    token = word_tokenize(s)\n",
    "    for t in token:\n",
    "        if t in set(stopwords.words('english')):\n",
    "            token.remove(t)\n",
    "    tokens.append(token)\n",
    "\n",
    "with open('tokens', 'wb') as f:\n",
    "    pickle.dump(tokens, f)\n",
    "\n",
    "'''\n",
    "with open ('tokens', 'rb') as f: #so the process need not be repeated in the future\n",
    "    tokens = pickle.load(f)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "    \n",
    "'''tagged = []\n",
    "for sent in tokens:\n",
    "    tagged.append(pos_tag(sent))\n",
    "wordnet_tagged = []\n",
    "for sent in tagged:\n",
    "    wordnet_tagged.append(list(map(lambda x: (x[0], pos_tagger(x[1])), sent)))\n",
    "print(wordnet_tagged)'''\n",
    "\n",
    "with open('wn_tagged', 'rb') as f:\n",
    "    wordnet_tagged = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "'''lemmatised = [] #lemmatization\n",
    "for sent in wordnet_tagged:\n",
    "    L = []\n",
    "    for i in sent:\n",
    "        if i[1] == None:\n",
    "            L.append(lemma.lemmatize(i[0]))\n",
    "        else:\n",
    "            L.append(lemma.lemmatize(*i))\n",
    "    lemmatised.append(L)'''\n",
    "\n",
    "with open('lemmatised', 'rb') as f:\n",
    "    lemmatised = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training, validation and test sentences before training **USING SMALLER DATASET AS CANNOT ALLOCATE RESOURCES**\n",
    "sents_train, sents_val, sents_test = lemmatised[:round(len(lemmatised)*0.1)], lemmatised[round(len(lemmatised)*0.1):round(len(lemmatised)*0.12)], lemmatised[round(len(lemmatised)*0.12):round(len(lemmatised)*0.14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectoriser():\n",
    "\n",
    "    def __init__(self, corpus=None):\n",
    "        self.word_set = {}\n",
    "        if corpus:\n",
    "            self.fit(corpus)\n",
    "    \n",
    "    def fit(self, corpus): #learns vocabulary of given corpus\n",
    "        ws = self.word_set\n",
    "        for d in corpus:\n",
    "            for t in d:\n",
    "                if t not in ws:\n",
    "                    ws[t] = len(ws)\n",
    "        self.word_set = ws\n",
    "    \n",
    "    def transform(self, doc): #returns feature vector for given document based on learned vocabulary\n",
    "        vec = np.zeros([len(self.word_set)], dtype=np.short) #generates vector of zeroes the same length as learned vocabulary\n",
    "        for t in doc:\n",
    "            if t in self.word_set:\n",
    "                vec[self.word_set[t]] += 1 #for every instance of a known word, add 1 to corresponding position in vector\n",
    "        return(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VVV = Vectoriser(sents_train) #fits vectoriser to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = np.array([VVV.transform(x) for x in sents_train], dtype=np.byte) #small datatype so that entire array can be created\n",
    "#val_vecs = np.array([VVV.transform(x) for x in sents_val], dtype=np.short)\n",
    "#test_vecs = np.array([VVV.transform(x) for x in sents_test], dtype=np.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"classifier = MultinomialNB()\\n\\nclassifier.fit(train_vecs, y_train)\\n\\n\\nwith open('trained_NB.pickle', 'rb') as f:\\n    classifier = pickle.load(f)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "y_train = labels[:round(len(labels)*0.1)]\n",
    "'''classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(train_vecs, y_train)\n",
    "\n",
    "\n",
    "with open('trained_NB.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055422264875239\n"
     ]
    }
   ],
   "source": [
    "y_val = labels[round(len(labels)*0.1):round(len(labels)*0.12)]\n",
    "val_vecs = np.array([VVV.transform(x) for x in sents_val], dtype=np.byte)\n",
    "preds = classifier.predict(val_vecs)\n",
    "\n",
    "total = 0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_val[i]:\n",
    "        total += 1\n",
    "\n",
    "accuracy = total/len(preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of first Naive bayes implementation: 80.55% (2 s.f.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pns = [[0,0,0] for i in range(6)]# 0: TP, 1: FP, 2: FN, TN can be inferred using other class TPs\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_val[i]:\n",
    "        val_pns[preds[i]][0] += 1 #increase TP count on predicted/true class\n",
    "    else:\n",
    "        val_pns[y_val[i]][2] += 1 #increase FN count on true class\n",
    "        val_pns[preds[i]][1] += 1 #increase FP count on predicted class\n",
    "\n",
    "#Going to use micro avg first\n",
    "total_tps = np.sum([x[0] for x in val_pns])\n",
    "total_fps = np.sum([x[1] for x in val_pns])\n",
    "total_fns = np.sum([x[2] for x in val_pns])\n",
    "\n",
    "precision = total_tps/(total_tps+total_fps)\n",
    "recall = total_tps/(total_tps+total_fns)\n",
    "\n",
    "fscore = 2*(precision*recall)/(precision+recall) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using micro averaging, precision, recall and f1-score are equal to accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_map = {\n",
    "    0 : \"Sadness\",\n",
    "    1 : \"Joy\",\n",
    "    2 : \"Love\",\n",
    "    3 : \"Anger\",\n",
    "    4 : \"Fear\",\n",
    "    5 : \"Surprise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadness: Precision = 0.7859116022099447, Recall = 0.9343185550082101, F1-Score = 0.8537134283570892\n",
      "Joy: Precision = 0.7786743515850144, Recall = 0.9414634146341463, F1-Score = 0.8523659305993689\n",
      "Love: Precision = 0.8589743589743589, Recall = 0.3901018922852984, F1-Score = 0.5365365365365365\n",
      "Anger: Precision = 0.9004474272930649, Recall = 0.7232704402515723, F1-Score = 0.802192326856004\n",
      "Fear: Precision = 0.8663911845730028, Recall = 0.6912087912087912, F1-Score = 0.7689486552567236\n",
      "Surprise: Precision = 0.9210526315789473, Recall = 0.109375, F1-Score = 0.19553072625698326\n",
      "0.6682146006437842\n"
     ]
    }
   ],
   "source": [
    "mac_f1 = f1_score(y_val, preds, average='macro')\n",
    "pr_re = []\n",
    "for x in val_pns:\n",
    "    pre = x[0]/(x[0]+x[1])\n",
    "    rec = x[0]/(x[0]+x[2])\n",
    "    f1 = 2*(pre*rec)/(pre+rec)\n",
    "    pr_re.append([pre, rec, f1])\n",
    "\n",
    "for i in range(len(pr_re)):\n",
    "    print(str(emo_map[i])+\": Precision =\", str(pr_re[i][0])+\", Recall =\", str(pr_re[i][1])+\", F1-Score =\", str(pr_re[i][2]))\n",
    "print(mac_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadness: Precision = 0.7859116022099447, Recall = 0.9343185550082101, F1-Score = 0.8537134283570892 <br>\n",
    "Joy: Precision = 0.7786743515850144, Recall = 0.9414634146341463, F1-Score = 0.8523659305993689 <br>\n",
    "Love: Precision = 0.8589743589743589, Recall = 0.3901018922852984, F1-Score = 0.5365365365365365 <br>\n",
    "Anger: Precision = 0.9004474272930649, Recall = 0.7232704402515723, F1-Score = 0.802192326856004 <br>\n",
    "Fear: Precision = 0.8663911845730028, Recall = 0.6912087912087912, F1-Score = 0.7689486552567236 <br>\n",
    "Surprise: Precision = 0.9210526315789473, Recall = 0.109375, F1-Score = 0.19553072625698326 <br>\n",
    "Macro averaged F1-Score = 0.6682146006437842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:  Surprise and Love missing a lot of tags, Fear and Anger missing some also <br>\n",
    "        Joy being over-classified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12265, 14059, 3409, 5688, 4720, 1540]\n",
      "41681\n"
     ]
    }
   ],
   "source": [
    "counts = [0 for i in range(6)]\n",
    "for i in y_train:\n",
    "    counts[i] += 1\n",
    "print(counts)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2436, 2870, 687, 1113, 910, 320]\n"
     ]
    }
   ],
   "source": [
    "counts = [0 for i in range(6)]\n",
    "\n",
    "for i in y_val:\n",
    "    counts[i] += 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'invite', 'dinner', 'my', 'bossman', 'friday', 'be', 'definitely', 'comfort', 'incredibly', 'invigorate', 'feel', 'be', 'some', 'pretty', 'tortured', 'experience', 'past', 'year']\n"
     ]
    }
   ],
   "source": [
    "counts = [0, 0, 0, 0, 0, 0]\n",
    "sents_train_2 = []\n",
    "train_labels_2 = []\n",
    "for i in range(len(lemmatised)):\n",
    "    if counts[labels[i]] < 7000:\n",
    "        sents_train_2.append(lemmatised[i])\n",
    "        train_labels_2.append(labels[i])\n",
    "        counts[labels[i]] += 1\n",
    "\n",
    "print(sents_train_2[3333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VVV_2 = Vectoriser(sents_train_2)\n",
    "\n",
    "train_vecs_2 = np.array([VVV.transform(x) for x in sents_train_2], np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_2 = MultinomialNB()\n",
    "\n",
    "classifier_2.fit(train_vecs_2, train_labels_2)\n",
    "\n",
    "with open('trained_NB_2.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 = classifier_2.predict(val_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_2 = accuracy_score(y_val, preds_2)\n",
    "print(acc_2)\n",
    "fs2 = f1_score(y_val, preds_2, average='macro')\n",
    "print(fs2)\n",
    "val_pns_2 = [[0,0,0] for i in range(6)]# 0: TP, 1: FP, 2: FN, TN can be inferred using other class TPs\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds_2[i] == y_val[i]:\n",
    "        val_pns_2[preds_2[i]][0] += 1 #increase TP count on predicted/true class\n",
    "    else:\n",
    "        val_pns_2[y_val[i]][2] += 1 #increase FN count on true class\n",
    "        val_pns_2[preds_2[i]][1] += 1 #increase FP count on predicted class\n",
    "\n",
    "pr_re_2 = []\n",
    "for x in val_pns_2:\n",
    "    pre = x[0]/(x[0]+x[1])\n",
    "    rec = x[0]/(x[0]+x[2])\n",
    "    f1 = 2*(pre*rec)/(pre+rec)\n",
    "    pr_re_2.append([pre, rec, f1])\n",
    "\n",
    "for i in range(len(pr_re_2)):\n",
    "    print(str(emo_map[i])+\": Precision =\", str(pr_re_2[i][0])+\", Recall =\", str(pr_re_2[i][1])+\", F1-Score =\", str(pr_re_2[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of Naive Bayes trained with equal numbers of each class in the training data\n",
    "\n",
    "0.8428502879078695 <br>\n",
    "0.8146509484657026<br>\n",
    "Sadness: Precision = 0.9391143911439115, Recall = 0.8357963875205254, F1-Score = 0.8844483058210252<br>\n",
    "Joy: Precision = 0.9633187772925764, Recall = 0.7686411149825784, F1-Score = 0.8550387596899225<br>\n",
    "Love: Precision = 0.6150190114068441, Recall = 0.9417758369723436, F1-Score = 0.7441058079355951<br>\n",
    "Anger: Precision = 0.8202068416865553, Recall = 0.9263252470799641, F1-Score = 0.870042194092827<br>\n",
    "Fear: Precision = 0.790258449304175, Recall = 0.8736263736263736, F1-Score = 0.8298538622129437<br>\n",
    "Surprise: Precision = 0.5523978685612788, Recall = 0.971875, F1-Score = 0.7044167610419025<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF START\n",
    "\n",
    "TF = Number of occurrences in document/total words in document\n",
    "\n",
    "IDF = log(total docs/docs with term)\n",
    "\n",
    "tf-idf = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_calc():\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.wordset = {}\n",
    "        self.total_docs = len(corpus)\n",
    "        self.fit(corpus)\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        ws = self.wordset\n",
    "        for d in corpus:\n",
    "            dws = set()         #create set to hold unique words present in document\n",
    "            for t in d:\n",
    "                if t not in dws:\n",
    "                    dws.add(t)\n",
    "            for w in dws:\n",
    "                if w not in ws:\n",
    "                    ws[w] = [len(ws), 1] #if word unseen, add to corpus word set\n",
    "                else:\n",
    "                    ws[w][1] += 1 #if word already seen, increase num of documents it has been seen in by 1\n",
    "        for w in ws:\n",
    "            ws[w][1] = np.log(len(corpus)/ws[w][1]) #convert doc frequency to idf values\n",
    "        self.wordset = ws\n",
    "    \n",
    "    def transform(self, doc):\n",
    "        vec = np.zeros((len(self.wordset)), dtype='float')\n",
    "        dws = {}\n",
    "        for w in doc:\n",
    "            if w in dws:\n",
    "                dws[w] += 1\n",
    "            else:\n",
    "                dws[w] = 1\n",
    "        for w in dws:\n",
    "            tf = dws[w]/len(doc)\n",
    "            if w in self.wordset:\n",
    "                vec[self.wordset[w][0]] = tf*self.wordset[w][1] \n",
    "        return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TV = TFIDF_calc(sents_train_2)\n",
    "\n",
    "train_vecs_3 = np.array([TV.transform(d) for d in sents_train_2])\n",
    "val_vecs_3 = np.array([TV.transform(d) for d in sents_val])\n",
    "y_val = labels[round(len(labels)*0.1):round(len(labels)*0.12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_3 = MultinomialNB()\n",
    "\n",
    "classifier_3.fit(train_vecs_3, train_labels_2)\n",
    "\n",
    "with open('trained_tfidf_NB.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3 = classifier_3.predict(val_vecs_3)\n",
    "\n",
    "acc_3 = accuracy_score(y_val, preds_3)\n",
    "f1_3 = f1_score(y_val, preds_3, average='macro')\n",
    "print(acc_3, f1_3)\n",
    "\n",
    "val_pns_3 = [[0,0,0] for i in range(6)]# 0: TP, 1: FP, 2: FN, TN can be inferred using other class TPs\n",
    "\n",
    "for i in range(len(preds_3)):\n",
    "    if preds_3[i] == y_val[i]:\n",
    "        val_pns_3[preds_3[i]][0] += 1 #increase TP count on predicted/true class\n",
    "    else:\n",
    "        val_pns_3[y_val[i]][2] += 1 #increase FN count on true class\n",
    "        val_pns_3[preds_3[i]][1] += 1 #increase FP count on predicted class\n",
    "\n",
    "pr_re_3 = []\n",
    "for x in val_pns_3:\n",
    "    pre = x[0]/(x[0]+x[1])\n",
    "    rec = x[0]/(x[0]+x[2])\n",
    "    f1 = 2*(pre*rec)/(pre+rec)\n",
    "    pr_re_3.append([pre, rec, f1])\n",
    "\n",
    "for i in range(len(pr_re_3)):\n",
    "    print(str(emo_map[i])+\": Precision =\", str(pr_re_3[i][0])+\", Recall =\", str(pr_re_3[i][1])+\", F1-Score =\", str(pr_re_3[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8495681381957774 0.8203670330670617<br>\n",
    "Sadness: Precision = 0.9418550992155053, Recall = 0.8378489326765188, F1-Score = 0.8868129480773408<br>\n",
    "Joy: Precision = 0.9528619528619529, Recall = 0.7888501742160279, F1-Score = 0.8631338162409454<br>\n",
    "Love: Precision = 0.631786771964462, Recall = 0.9315866084425036, F1-Score = 0.7529411764705882<br>\n",
    "Anger: Precision = 0.8469135802469135, Recall = 0.9245283018867925, F1-Score = 0.8840206185567011<br>\n",
    "Fear: Precision = 0.7897838899803536, Recall = 0.8835164835164835, F1-Score = 0.8340248962655602<br>\n",
    "Surprise: Precision = 0.5557586837294333, Recall = 0.95, F1-Score = 0.7012687427912341 <br>\n",
    "\n",
    "NOTES: TF-IDF vectors slightly increased Accuracy and F1-Score, Main issue is again love and surprise, but now they are being overclassified. Maybe use training dataset with the log of the true proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "true = [0 for i in range (6)]\n",
    "for i in y_train:\n",
    "    true[i] += 1\n",
    "\n",
    "plt.bar([i for i in range(6)], true)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio in simplest form: 12265 : 14059 : 3409 : 5688 : 4720 : 1540 <br>\n",
    "Log of ratio: 9415 : 9551 : 8134 : 8646 : 8460 : 7340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ratio = [9415, 9551, 8134, 8646, 8460, 7340] #getting training dataset with the proportion of each classes equal to the log of the true proportions\n",
    "\n",
    "log_ratio = [np.round(i*0.8) for i in log_ratio]\n",
    "\n",
    "counts = [0, 0, 0, 0, 0, 0] #\n",
    "sents_train_4 = []\n",
    "train_labels_4 = []\n",
    "\n",
    "for i in range(len(lemmatised)):\n",
    "    if counts[labels[i]] < log_ratio[labels[i]]:\n",
    "        sents_train_4.append(lemmatised[i])\n",
    "        train_labels_4.append(labels[i])\n",
    "        counts[labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41681\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_2 = TFIDF_calc(sents_train_4)\n",
    "\n",
    "train_vecs_4 = np.array([tfidf_2.transform(x) for x in sents_train_4])\n",
    "\n",
    "classifier_4 = MultinomialNB()\n",
    "classifier_4.fit(train_vecs_4, np.array(train_labels_4))\n",
    "\n",
    "with open('tfidf_2.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier_4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vecs_4 = np.array([tfidf_2.transform(x) for x in sents_val])\n",
    "preds_4 = classifier_4.predict(val_vecs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8738003838771593 0.8521126060748608\n",
      "Sadness: Precision = 0.9250757903854483, Recall = 0.8768472906403941, F1-Score = 0.9003161222339305\n",
      "Joy: Precision = 0.933718689788054, Recall = 0.8442508710801394, F1-Score = 0.8867337602927722\n",
      "Love: Precision = 0.6908267270668177, Recall = 0.8879184861717613, F1-Score = 0.7770700636942676\n",
      "Anger: Precision = 0.8666666666666667, Recall = 0.9227313566936208, F1-Score = 0.8938207136640557\n",
      "Fear: Precision = 0.8216106014271152, Recall = 0.8857142857142857, F1-Score = 0.8524590163934426\n",
      "Surprise: Precision = 0.7362924281984334, Recall = 0.88125, F1-Score = 0.8022759601706971\n"
     ]
    }
   ],
   "source": [
    "acc_4 = accuracy_score(y_val, preds_4)\n",
    "f1_4 = f1_score(y_val, preds_4, average='macro')\n",
    "print(acc_4, f1_4)\n",
    "\n",
    "val_pns_4 = [[0,0,0] for i in range(6)]# 0: TP, 1: FP, 2: FN, TN can be inferred using other class TPs\n",
    "\n",
    "for i in range(len(preds_4)):\n",
    "    if preds_4[i] == y_val[i]:\n",
    "        val_pns_4[preds_4[i]][0] += 1 #increase TP count on predicted/true class\n",
    "    else:\n",
    "        val_pns_4[y_val[i]][2] += 1 #increase FN count on true class\n",
    "        val_pns_4[preds_4[i]][1] += 1 #increase FP count on predicted class\n",
    "\n",
    "pr_re_4 = []\n",
    "for x in val_pns_4:\n",
    "    pre = x[0]/(x[0]+x[1])\n",
    "    rec = x[0]/(x[0]+x[2])\n",
    "    f1 = 2*(pre*rec)/(pre+rec)\n",
    "    pr_re_4.append([pre, rec, f1])\n",
    "\n",
    "for i in range(len(pr_re_4)):\n",
    "    print(str(emo_map[i])+\": Precision =\", str(pr_re_4[i][0])+\", Recall =\", str(pr_re_4[i][1])+\", F1-Score =\", str(pr_re_4[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8738003838771593 0.8521126060748608 <br>\n",
    "Sadness: Precision = 0.9250757903854483, Recall = 0.8768472906403941, F1-Score = 0.9003161222339305<br>\n",
    "Joy: Precision = 0.933718689788054, Recall = 0.8442508710801394, F1-Score = 0.8867337602927722<br>\n",
    "Love: Precision = 0.6908267270668177, Recall = 0.8879184861717613, F1-Score = 0.7770700636942676<br>\n",
    "Anger: Precision = 0.8666666666666667, Recall = 0.9227313566936208, F1-Score = 0.8938207136640557<br>\n",
    "Fear: Precision = 0.8216106014271152, Recall = 0.8857142857142857, F1-Score = 0.8524590163934426<br>\n",
    "Surprise: Precision = 0.7362924281984334, Recall = 0.88125, F1-Score = 0.8022759601706971<br>\n",
    "NOTES: Using the log of the true ratios seems to provide a good optimum ratio (there are still roughly the same number of total data points in the training set). Accuracy increased by a further 2% while f1-score increased by about 3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
