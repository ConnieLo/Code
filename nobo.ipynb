{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 4 ... 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "#separate data into text and labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('text.csv')\n",
    "\n",
    "text = []\n",
    "labels = []\n",
    "for row in df.iterrows():\n",
    "    text.append(row[1]['text'])\n",
    "    labels.append(row[1]['label'])\n",
    "\n",
    "labels = np.array(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import preprocessing module(s), tokenise etc\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "'''tokens = []\n",
    "for s in text:\n",
    "    s.maketrans('','', string.punctuation)\n",
    "    token = word_tokenize(s)\n",
    "    for t in token:\n",
    "        if t in set(stopwords.words('english')):\n",
    "            token.remove(t)\n",
    "    tokens.append(token)\n",
    "\n",
    "with open('tokens', 'wb') as f:\n",
    "    pickle.dump(tokens, f)\n",
    "\n",
    "'''\n",
    "with open ('tokens', 'rb') as f: #so the process need not be repeated in the future\n",
    "    tokens = pickle.load(f)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "    \n",
    "'''tagged = []\n",
    "for sent in tokens:\n",
    "    tagged.append(pos_tag(sent))\n",
    "wordnet_tagged = []\n",
    "for sent in tagged:\n",
    "    wordnet_tagged.append(list(map(lambda x: (x[0], pos_tagger(x[1])), sent)))\n",
    "print(wordnet_tagged)'''\n",
    "\n",
    "with open('wn_tagged', 'rb') as f:\n",
    "    wordnet_tagged = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "'''lemmatised = [] #lemmatization\n",
    "for sent in wordnet_tagged:\n",
    "    L = []\n",
    "    for i in sent:\n",
    "        if i[1] == None:\n",
    "            L.append(lemma.lemmatize(i[0]))\n",
    "        else:\n",
    "            L.append(lemma.lemmatize(*i))\n",
    "    lemmatised.append(L)'''\n",
    "\n",
    "with open('lemmatised', 'rb') as f:\n",
    "    lemmatised = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training, validation and test sentences before training **USING SMALLER DATASET AS CANNOT ALLOCATE RESOURCES**\n",
    "sents_train, sents_val, sents_test = lemmatised[:round(len(lemmatised)*0.1)], lemmatised[round(len(lemmatised)*0.1):round(len(lemmatised)*0.12)], lemmatised[round(len(lemmatised)*0.12):round(len(lemmatised)*0.14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectoriser():\n",
    "\n",
    "    def __init__(self, corpus=None):\n",
    "        self.word_set = {}\n",
    "        if corpus:\n",
    "            self.fit(corpus)\n",
    "    \n",
    "    def fit(self, corpus): #learns vocabulary of given corpus\n",
    "        ws = self.word_set\n",
    "        for d in corpus:\n",
    "            for t in d:\n",
    "                if t not in ws:\n",
    "                    ws[t] = len(ws)\n",
    "        self.word_set = ws\n",
    "    \n",
    "    def transform(self, doc): #returns feature vector for given document based on learned vocabulary\n",
    "        vec = np.zeros([len(self.word_set)], dtype=np.short) #generates vector of zeroes the same length as learned vocabulary\n",
    "        for t in doc:\n",
    "            if t in self.word_set:\n",
    "                vec[self.word_set[t]] += 1 #for every instance of a known word, add 1 to corresponding position in vector\n",
    "        return(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VVV = Vectoriser(sents_train) #fits vectoriser to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = np.array([VVV.transform(x) for x in sents_train], dtype=np.byte) #small datatype so that entire array can be created\n",
    "#val_vecs = np.array([VVV.transform(x) for x in sents_val], dtype=np.short)\n",
    "#test_vecs = np.array([VVV.transform(x) for x in sents_test], dtype=np.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "y_train = labels[:round(len(labels)*0.1)]\n",
    "'''classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(train_vecs, y_train)\n",
    "\n",
    "'''\n",
    "with open('trained_NB.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055422264875239\n"
     ]
    }
   ],
   "source": [
    "y_val = labels[round(len(labels)*0.1):round(len(labels)*0.12)]\n",
    "val_vecs = np.array([VVV.transform(x) for x in sents_val], dtype=np.byte)\n",
    "preds = classifier.predict(val_vecs)\n",
    "\n",
    "total = 0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_val[i]:\n",
    "        total += 1\n",
    "\n",
    "accuracy = total/len(preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of first Naive bayes implementation: 80.55% (2 s.f.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pns = [[0,0,0] for i in range(6)]# 0: TP, 1: FP, 2: FN, TN can be inferred using other class TPs\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_val[i]:\n",
    "        val_pns[preds[i]][0] += 1 #increase TP count on predicted/true class\n",
    "    else:\n",
    "        val_pns[y_val[i]][2] += 1 #increase FN count on true class\n",
    "        val_pns[preds[i]][1] += 1 #increase FP count on predicted class\n",
    "\n",
    "#Going to use micro avg first\n",
    "total_tps = np.sum([x[0] for x in val_pns])\n",
    "total_fps = np.sum([x[1] for x in val_pns])\n",
    "total_fns = np.sum([x[2] for x in val_pns])\n",
    "\n",
    "precision = total_tps/(total_tps+total_fps)\n",
    "recall = total_tps/(total_tps+total_fns)\n",
    "\n",
    "fscore = 2*(precision*recall)/(precision+recall) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using micro averaging, precision, recall and f1-score are equal to accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadness: Precision = 0.7859116022099447, Recall = 0.9343185550082101, F1-Score = 0.8537134283570892\n",
      "Joy: Precision = 0.7786743515850144, Recall = 0.9414634146341463, F1-Score = 0.8523659305993689\n",
      "Love: Precision = 0.8589743589743589, Recall = 0.3901018922852984, F1-Score = 0.5365365365365365\n",
      "Anger: Precision = 0.9004474272930649, Recall = 0.7232704402515723, F1-Score = 0.802192326856004\n",
      "Fear: Precision = 0.8663911845730028, Recall = 0.6912087912087912, F1-Score = 0.7689486552567236\n",
      "Surprise: Precision = 0.9210526315789473, Recall = 0.109375, F1-Score = 0.19553072625698326\n",
      "0.6682146006437842\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "emo_map = {\n",
    "    0 : \"Sadness\",\n",
    "    1 : \"Joy\",\n",
    "    2 : \"Love\",\n",
    "    3 : \"Anger\",\n",
    "    4 : \"Fear\",\n",
    "    5 : \"Surprise\"\n",
    "}\n",
    "\n",
    "mac_f1 = f1_score(y_val, preds, average='macro')\n",
    "pr_re = []\n",
    "for x in val_pns:\n",
    "    pre = x[0]/(x[0]+x[1])\n",
    "    rec = x[0]/(x[0]+x[2])\n",
    "    f1 = 2*(pre*rec)/(pre+rec)\n",
    "    pr_re.append([pre, rec, f1])\n",
    "\n",
    "for i in range(len(pr_re)):\n",
    "    print(str(emo_map[i])+\": Precision =\", str(pr_re[i][0])+\", Recall =\", str(pr_re[i][1])+\", F1-Score =\", str(pr_re[i][2]))\n",
    "print(mac_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadness: Precision = 0.7859116022099447, Recall = 0.9343185550082101, F1-Score = 0.8537134283570892 <br>\n",
    "Joy: Precision = 0.7786743515850144, Recall = 0.9414634146341463, F1-Score = 0.8523659305993689 <br>\n",
    "Love: Precision = 0.8589743589743589, Recall = 0.3901018922852984, F1-Score = 0.5365365365365365 <br>\n",
    "Anger: Precision = 0.9004474272930649, Recall = 0.7232704402515723, F1-Score = 0.802192326856004 <br>\n",
    "Fear: Precision = 0.8663911845730028, Recall = 0.6912087912087912, F1-Score = 0.7689486552567236 <br>\n",
    "Surprise: Precision = 0.9210526315789473, Recall = 0.109375, F1-Score = 0.19553072625698326 <br>\n",
    "Macro averaged F1-Score = 0.6682146006437842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:  Surprise and Love missing a lot of tags, Fear and Anger missing some also <br>\n",
    "        Joy being over-classified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12265, 14059, 3409, 5688, 4720, 1540]\n",
      "41681\n"
     ]
    }
   ],
   "source": [
    "counts = [0 for i in range(6)]\n",
    "for i in y_train:\n",
    "    counts[i] += 1\n",
    "print(counts)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2436, 2870, 687, 1113, 910, 320]\n"
     ]
    }
   ],
   "source": [
    "counts = [0 for i in range(6)]\n",
    "\n",
    "for i in y_val:\n",
    "    counts[i] += 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'invite', 'dinner', 'my', 'bossman', 'friday', 'be', 'definitely', 'comfort', 'incredibly', 'invigorate', 'feel', 'be', 'some', 'pretty', 'tortured', 'experience', 'past', 'year']\n"
     ]
    }
   ],
   "source": [
    "counts = [0, 0, 0, 0, 0, 0]\n",
    "sents_train_2 = []\n",
    "train_labels_2 = []\n",
    "for i in range(len(lemmatised)):\n",
    "    if counts[labels[i]] < 7000:\n",
    "        sents_train_2.append(lemmatised[i])\n",
    "        train_labels_2.append(labels[i])\n",
    "        counts[labels[i]] += 1\n",
    "\n",
    "print(sents_train_2[3333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VVV_2 = Vectoriser(sents_train_2)\n",
    "\n",
    "train_vecs_2 = np.array([VVV.transform(x) for x in sents_train_2], np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_2 = MultinomialNB()\n",
    "\n",
    "classifier_2.fit(train_vecs_2, train_labels_2)\n",
    "\n",
    "with open('trained_NB_2.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier_2, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
